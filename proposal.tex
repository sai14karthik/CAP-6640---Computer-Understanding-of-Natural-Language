
%%
%% This is file `proposal.tex' based on ACM Hypertext Conference Template
%%
\documentclass[sigconf]{acmart}

%%
%% Remove ACM-specific copyright and conference information for proposal
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

%%
%% Title and author information
\title{Hallucination Detection and Measurement in Large Language Models: A Comparative Analysis on Factual Question Answering}

\author{Sai Karthik Nallamothu}
\affiliation{%
  \institution{University of Central Florida}
  \city{Orlando}
  \state{FL}
  \country{USA}
}

%%
%% Begin document
\begin{document}

%%
%% Generate title
\maketitle

%%
%% Section 1: Problem Statement
\section{Problem Statement}

Over the past few years, Large Language Models (LLMs) have demonstrated strong performance across a wide range of natural language processing tasks in real-world applications such as chatbots, virtual assistants, and many more. Despite these advances, LLMs sometimes exhibit hallucinations, where the model generates factually incorrect information that is misleading or unsupported by any evidence. This issue is particularly critical in tasks such as factual question answering, where users expect responses to be accurate, reliable, and trustworthy.

Previous works have studied hallucinations in LLMs, but there remains a lack of systematic and comparative analysis across multiple open-source models on various factual QA datasets and different prompting methods. There is a significant gap in our understanding of how hallucination behavior varies across different architectures, model sizes, and question types. Moreover, the relationship between prompt design and hallucination occurrence has not been thoroughly explored. Addressing these gaps is important as open-source LLMs are increasingly deployed in fact-critical areas such as education, medicine, search engines, and many more. The goal of this project is to fill these gaps by conducting a comprehensive evaluation of hallucination detection and measurement across a broad ecosystem of open-source LLMs on factual QA tasks, with a specific focus on prompt sensitivity and model reasoning behavior.

%%
%% Section 2: Proposed Technique
\section{Proposed Technique}

In this project, I will perform a thorough evaluation of hallucination detection and measurement using a systematic experimental framework. The methodology contains four main components:

\textbf{Datasets:} I will evaluate the open-source models on three to five benchmark datasets focused on factual question answering, including TruthfulQA (general knowledge and misconceptions), WikiQA (Wikipedia-based factual questions), Natural Questions (real user questions from Google), FEVER (fact verification claims), and SQuAD 2.0 (reading comprehension with unanswerable questions). These datasets provide a wide range of question types and domains for comprehensive evaluation. Since these datasets are large in volume, a suitable subset of each dataset will be used to benchmark the open-source models.

\textbf{Models:} For this project, I plan to test five to six open-source models such as Llama 3, Mistral, Phi-2, Gemma, Llama 2, and Qwen. All these models can be imported through the Hugging Face Transformers library. The mentioned models cover different architectures and sizes for complete benchmarking.

\textbf{Evaluation Framework:} For each model-dataset combination, I will measure accuracy by comparing model responses with ground truth answers, compute hallucination rates using automated fact-checking and consistency analysis, analyze the relationship between prompt design (zero-shot and few-shot prompting) and hallucination occurrence frequency, and identify question types and domains that are most susceptible to hallucinations through error investigation. This framework will be implemented using Python with Hugging Face Transformers, datasets, and evaluation libraries like scikit-learn.

\textbf{Metrics:} To evaluate the open-source models' performance, I will use accuracy, precision, recall, hallucination rate (percentage of fabricated or incorrect content), and confidence-accuracy correlation to assess model reliability. Statistical analysis will be performed to compare performance across models and prompt strategies using appropriate significance tests.

%%
%% Section 3: Expected Outcomes
\section{Expected Outcomes}

This project will produce several practical and achievable outcomes. First, it will provide a comprehensive comparative analysis of hallucination patterns across multiple open-source models, highlighting which models are most prone to hallucinations and identifying the conditions (such as dataset attributes and question complexity types) where failures are most likely to occur. Second, this project will present quantitative measurements of hallucination rates for each model-dataset pair, establishing scalable baseline metrics that can support future benchmarking and comparisons as models continue to evolve.

Third, this project will provide evidence-based insights into how different prompt design strategies (such as zero-shot and few-shot prompting) can influence or control hallucination behavior in LLMs, offering practical guidance for prompt engineering in factual question answering tasks. Fourth, through detailed error and domain-level analysis, the study will identify specific question types (such as unanswerable questions and ambiguous queries) and knowledge domains that are particularly susceptible to hallucinations, enabling more targeted reduction methods. Finally, based on the experimental findings, I will propose actionable recommendations for reducing hallucinations in factual QA systems, including model selection guidelines and best practices for prompt engineering.

These findings will help us understand the limitations of current LLMs, establish best practices for deploying open-source models in fact-critical applications, and provide a strong methodological foundation for developing more reliable and trustworthy NLP applications. The results will be especially valuable for AI engineers and practitioners aiming to deploy LLMs in real-world applications where factual accuracy and user trust are vital. Through this project, I aim to contribute a new study that addresses overall gaps in this domain, which is feasible within a semester timeframe.

%%
%% Section 4: Topic Number
\section{Topic Number}

\textbf{Topic \#3} from Group 1: Benchmarking and Evaluation of NLP Systems - Hallucination Detection and Measurement in LLMs.

%%
%% End of document
\end{document}
